#!/usr/bin/env python
# -*- coding: utf-8 -*-

import math
import torch
import torch.nn as nn
from tqdm import tqdm
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model
import evaluate as eval_metric
from sklearn.model_selection import train_test_split

# -----------------------------
# 0. Настройки
# -----------------------------
MODEL_NAME = "distilgpt2"
MAX_LENGTH = 128
BATCH_SIZE = 16
GRAD_ACCUM_STEPS = 2
NUM_EPOCHS = 3
LEARNING_RATE = 5e-5
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------------
# 1. Данные
# -----------------------------
# df_clean должен содержать колонку 'text'
texts = df_clean['text'].tolist()
train_texts, val_texts = train_test_split(texts, test_size=0.05, random_state=42)

train_dataset = Dataset.from_dict({"text": train_texts})
val_dataset = Dataset.from_dict({"text": val_texts})

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize(batch):
    encodings = tokenizer(batch["text"], padding='max_length', truncation=True, max_length=MAX_LENGTH)
    encodings["labels"] = encodings["input_ids"].copy()
    return encodings

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)

# -----------------------------
# 2. QLoRA + LoRA
# -----------------------------
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["c_proj","c_attn"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto"
)
model = get_peft_model(model, lora_config)

# -----------------------------
# 3. Аргументы обучения
# -----------------------------
training_args = TrainingArguments(
    output_dir="./lora_gpt2",
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM_STEPS,
    num_train_epochs=NUM_EPOCHS,
    learning_rate=LEARNING_RATE,
    logging_steps=200,
    save_steps=2000,
    eval_steps=2000,
    eval_strategy="steps",
    save_strategy="steps",
    save_total_limit=2,
    fp16=True,
    load_best_model_at_end=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
)

# -----------------------------
# 4. Обучение
# -----------------------------
trainer.train()
model.save_pretrained("./lora_gpt2")
tokenizer.save_pretrained("./lora_gpt2")

# -----------------------------
# 5. Генерация текста
# -----------------------------
model.to(DEVICE)
model.eval()

def generate_text(prompt, max_length=50, temperature=0.7, top_p=0.9):
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(DEVICE)
    output = model.generate(
        input_ids,
        max_length=max_length,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(output[0], skip_special_tokens=True).strip()

# -----------------------------
# 6. ROUGE и Perplexity
# -----------------------------
rouge = eval_metric.load("rouge")
loss_fct = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction="sum")

def evaluate_model(prompts, references):
    hypotheses = []
    refs = []
    total_loss = 0.0
    total_tokens = 0

    for prompt, ref in tqdm(zip(prompts, references), total=len(prompts)):
        gen_text = generate_text(prompt)
        hypotheses.append(gen_text)
        refs.append(ref)

        enc = tokenizer(prompt + " " + ref, return_tensors="pt").to(DEVICE)
        input_ids = enc["input_ids"]
        with torch.no_grad():
            outputs = model(input_ids, labels=input_ids)
            shift_logits = outputs.logits[..., :-1, :].contiguous()
            shift_labels = input_ids[..., 1:].contiguous()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            total_loss += loss.item()
            total_tokens += shift_labels.numel()

    avg_loss = total_loss / total_tokens
    perplexity = math.exp(avg_loss)
    rouge_scores = rouge.compute(predictions=hypotheses, references=refs)

    return rouge_scores, perplexity, list(zip(prompts, refs, hypotheses))

# -----------------------------
# 7. Пример использования
# -----------------------------
test_examples = [
    ("I love", "I love this movie so much"),
    ("The weather is", "The weather is beautiful today"),
    ("What do you think about", "What do you think about this idea")
]

prompts = [ex[0] for ex in test_examples]
references = [ex[1] for ex in test_examples]

rouge_results, ppl, examples = evaluate_model(prompts, references)

print("\nROUGE scores:", rouge_results)
print(f"Perplexity: {ppl:.2f}")
for prompt, ref, gen in examples:
    print(f"\nPrompt: {prompt}\nReference: {ref}\nGenerated: {gen}")
